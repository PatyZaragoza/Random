#!/usr/bin/env python
# coding: utf-8

# # Caso practico: Random forest
# 
# En este caso de uso practico se pretende resolver un problema de deteccion de Malware en dispositivos Android mediante el analisis del trafico de red que gerena el dispositivo mediante eñ uso de subconjunto de arboles de desicion 
# 
# ### DataSet: Deteccion del Malaware en Android 
# 
# #### Descripcion:
# The sophisticated and advanced Android malware is able to identify the presence of the emulator used by the malware analyst and in response, alter its behaviour to evade detection. To overcome this issue, we installed the Android applications on the real device and captured its network traffic. 
# 
# CICAAGM dataset is captured by installing the Android apps on the real smartphones semi-automated. The dataset is generated from 1,900 applications with the following three categories:
# 1. Adware (250 apps)
# 
#     Airpush: Designed to deliver unsolicited advertisements to the user’s systems for information stealing.
# 
#     Dowgin: Designed as an advertisement library that can also steal the user’s information.
# 
#     Kemoge: Designed to take over a user’s Android device. This adware is a hybrid of botnet and disguises itself as popular apps via repackaging.
# 
#     Mobidash: Designed to display ads and to compromise user’s personal information.
# 
#     Shuanet: Similar to Kemoge, Shuanet is also designed to take over a user’s device.
# 
# 2. General Malware (150 apps)
# 
#     AVpass: Designed to be distributed in the guise of a Clock app.
# 
#     FakeAV: Designed as a scam that tricks user to purchase a full version of the software in order to re-mediate non-existing infections.
# 
#     FakeFlash/FakePlayer: Designed as a fake Flash app in order to direct users to a website (after successfully installed).
# 
#     GGtracker: Designed for SMS fraud (sends SMS messages to a premium-rate number) and information stealing.
# 
#     Penetho: Designed as a fake service (hacktool for Android devices that can be used to crack the WiFi password). The malware is also able to infect the user’s computer via infected email attachment, fake updates, external media and infected documents.
# 
# 3. Benign (1,500 apps)
# 
#     2015 GooglePlay market (top free popular and top free new)
# 
#     2016 GooglePlay market (top free popular and top free new)
# 
# License
# 
# The CICAAGM dataset consists of the following items is publicly available for researchers.
# 
#     .pcap files – the network traffic of both the malware and benign (20% malware and 80% benign)
# 
#     .csv files - the list of extracted network traffic features generated by the CIC-flowmeter
# 
# If you are using our dataset, you should cite our related paper that outlines the details of the dataset and its underlying principles:
# 
#     Arash Habibi Lashkari, Andi Fitriah A. Kadir, Hugo Gonzalez, Kenneth Fon Mbah and Ali A. Ghorbani, “Towards a Network-Based Framework for Android Malware Detection and Characterization”, In the proceeding of the 15th International Conference on Privacy, Security and Trust, PST, Calgary, Canada, 2017.
# 

# ## Imports 

# In[1]:


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import f1_score
from pandas import DataFrame

# In[2]:


# construccion de una funcion que realice el particionado completo.
def train_val_test_split(df, rstate = 42,shuffle = True, stratify=None):
    strat = df[stratify] if stratify else None 
    train_set, test_set = train_test_split(
        df, test_size = 0.4, random_state = rstate, shuffle=shuffle, stratify = strat)
    strat = train_test[stratify] if stratify else None
    val_set, test_set = train_test_split(
        test_set, test_size= 0.5, random_state = rstate, shuffle = shuffle, stratify = strat)
    return (train_set, val_set, test_set)

# In[3]:


def remove_labels(df, label_name):
    X = df.drop(label_name, axis =1)
    y = df[label_name].copy()
    return (X, y)

# In[4]:


def evaluate_result(y_pred, y, y_prep_pred, y_prep, metric):
    print(metric.__name__,"WITHOUT preparation:", metric(y_pred, y, average="weighted"))
    print(metric.__name__,"WITH preparation:", metric(y_prep_pred, y_prep, average="weighted"))
    

# ## 1.- Lectura de DataSet

# In[5]:


df = pd.read_csv("Android/TotalFeatures-ISCXFlowMeter.csv")

# ## 2.- Visualizacion de DataSet

# In[6]:


df.head(10)

# In[7]:


df.describe()

# In[8]:


df.info()

# In[9]:


print("Longitud del DataSet:", len(df))
print("Numero de caracteristicas del DataSet:", len(df.columns))

# In[10]:


df["calss"].value_counts()

# #### Buscando correlaciones

# In[11]:


# Tranformamos la variale de salida a numerica para colculas correlaciones

X = df.copy()
X['calss'] = X['calss'].factorize()[0]


# In[12]:


# Calcular las correlaciones
corr_matrix = X.corr()
corr_matrix["calss"].sort_values(ascending = False)

# In[13]:


X.corr()

# In[14]:


# se peude llegar a valorar yqudarnos con las aquellas que tenga mayor correlacion
corr_matrix[corr_matrix['calss'] > 0.05]

# # 3.- Division del DataSet

# In[15]:


# Dividir el DataSet
train_set, val_set, test_set= train_val_test_split(df)

# In[16]:


X_train, y_train = remove_labels(train_set, 'calss')
X_val, y_val = remove_labels(val_set, 'calss')
X_test, y_test = remove_labels(test_set, 'calss')

# # 4.-Escalado del DataSet
# 
# E importante comprender que los arboles de decision son algoritmos que **no requieren demaciada preparacion de los datos** correctamente, no requieren la realizacion o escalado o normalizacion. En este ejercicio se va a realizar escalado al DatSet y se van a compara los resultados con el conjunto de datos sin escalar, de esta manera se demuestra como aplicar preprocesamientos como el escalado puede llegar a aafectar el rendimiento del modelo 

# In[17]:


scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)

# In[18]:


scaler = RobustScaler()
X_test_scaled = scaler.fit_transform(X_test)

# In[19]:


scaler = RobustScaler()
X_val_scaled = scaler.fit_transform(X_val)

# In[20]:


X_train_scaled = DataFrame(X_train_scaled, columns = X_train.columns, index=X_train.index)
X_train_scaled.head(10)

# In[21]:


X_train_scaled.describe()

# ## 5.- Decision Forest

# In[22]:


# Modelo entrenado con el DatSet sin escalar

from sklearn.tree import DecisionTreeClassifier

clf_tree = DecisionTreeClassifier(random_state = 42)
clf_tree.fit(X_train, y_train)

# In[23]:


# Predecir con el DataSet de entrenamiento 
y_train_pred =  clf_tree.predict(X_train)

# In[24]:


print("F1_Score train Set:", f1_score(y_train_pred, y_train, average="weighted"))

# In[25]:


# Predecir con el DatSet de validacipn
y_val_pred = clf_tree.predict(X_val)

# In[26]:


# coparara resulados entre el escalado y sin escalar
print("F1 Score Validation set:", f1_score(y_val_pred, y_val, average="weighted"))

# # 6.- Random Forest

# In[27]:


from sklearn.ensemble import RandomForestClassifier

# Modelo entrenado co el DtaSet sin escalr
clf_rnd = RandomForestClassifier(n_estimators =100, random_state = 42, n_jobs= -1)
clf_rnd.fit(X_train, y_train)

# In[28]:


# Modelo de entrenamiento con el DataSet escalado
clf_rnd_scaled = RandomForestClassifier(n_estimators =100, random_state = 42, n_jobs= -1)
clf_rnd_scaled.fit(X_train_scaled, y_train)

# In[29]:


# predecir con el DataSet de entrenamiento
y_train_pred = clf_rnd.predict(X_train)
y_train_prep_pred= clf_rnd_scaled.predict(X_train_scaled)

# In[30]:


# Comparar resultados entre el escalado y sin escalar
evaluate_result(y_train_pred, y_train, y_train_prep_pred, y_train, f1_score)

# # 7.- Regresion Forest

# In[31]:


from sklearn.ensemble import RandomForestRegressor

# In[32]:


# Comprobacion de, si existen valores nulos
is_null = df.isna().any()
is_null[is_null]

# In[33]:


print("calss" in X_train.columns)


# In[35]:


df["calss"].value_counts()

# In[36]:


# Dividir en características (X) y la variable objetivo (y)
X = df.drop(columns=['calss'])
y = df['calss']


# In[37]:


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# In[42]:


import pandas as pd
from sklearn.preprocessing import LabelEncoder, RobustScaler

# Tomar una muestra de 5000 datos si quieres trabajar con una menor cantidad
df_sample = df.sample(n=5000, random_state=42)

# Convertir las columnas categóricas a numéricas
label_encoder = LabelEncoder()
df_sample['calss'] = label_encoder.fit_transform(df_sample['calss'])

# Separar las características (X) y la variable objetivo (y)
X = df_sample.drop(columns=['calss'])  # Sustituye 'calss' con el nombre de tu variable objetivo si es diferente
y = df_sample['calss']

# Escalar las características con RobustScaler
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)

# Crear un DataFrame de las características escaladas para visualizar el resultado
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# Verifica los primeros datos escalados
print(X_scaled_df.head())


# In[ ]:




# In[ ]:



